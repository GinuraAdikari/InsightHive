{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcrpMZLfwnWm10Fp80CKiH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GinuraAdikari/InsightHive/blob/main/model_ATE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiEFnTvJG8BZ",
        "outputId": "30a04bed-9f4c-4baa-add7-bd2f5994b189"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "TW_KXb04iGy4",
        "outputId": "1c9e3214-4c2e-4faf-baf5-3051b0bac16d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     No.                                        Review_Text  Sentiment_Label  \\\n",
              "0      1  one best game music soundtracks game didnt rea...                1   \n",
              "1  10001  best purchase ever bought exersaucer little gu...                1   \n",
              "2  20001  book slow weak one beststhank god slow weak bo...                0   \n",
              "3  30001  mustread every southern lady failed love book ...                1   \n",
              "4  40001  horrible watch napoleon want funny movie sucks...                0   \n",
              "\n",
              "   review_length                                             tokens  \\\n",
              "0             79  ['one', 'best', 'game', 'music', 'soundtrack',...   \n",
              "1             39  ['best', 'purchase', 'ever', 'bought', 'exersa...   \n",
              "2             20  ['book', 'slow', 'weak', 'one', 'beststhank', ...   \n",
              "3             49  ['mustread', 'every', 'southern', 'lady', 'fai...   \n",
              "4             32  ['horrible', 'watch', 'napoleon', 'want', 'fun...   \n",
              "\n",
              "                                      Cleaned_review  \n",
              "0  one best game music soundtrack game didnt real...  \n",
              "1  best purchase ever bought exersaucer little gu...  \n",
              "2  book slow weak one beststhank god slow weak bo...  \n",
              "3  mustread every southern lady failed love book ...  \n",
              "4  horrible watch napoleon want funny movie suck ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5026048-7feb-43ba-b4a8-bf9ea11f861e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No.</th>\n",
              "      <th>Review_Text</th>\n",
              "      <th>Sentiment_Label</th>\n",
              "      <th>review_length</th>\n",
              "      <th>tokens</th>\n",
              "      <th>Cleaned_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>one best game music soundtracks game didnt rea...</td>\n",
              "      <td>1</td>\n",
              "      <td>79</td>\n",
              "      <td>['one', 'best', 'game', 'music', 'soundtrack',...</td>\n",
              "      <td>one best game music soundtrack game didnt real...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001</td>\n",
              "      <td>best purchase ever bought exersaucer little gu...</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>['best', 'purchase', 'ever', 'bought', 'exersa...</td>\n",
              "      <td>best purchase ever bought exersaucer little gu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20001</td>\n",
              "      <td>book slow weak one beststhank god slow weak bo...</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>['book', 'slow', 'weak', 'one', 'beststhank', ...</td>\n",
              "      <td>book slow weak one beststhank god slow weak bo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30001</td>\n",
              "      <td>mustread every southern lady failed love book ...</td>\n",
              "      <td>1</td>\n",
              "      <td>49</td>\n",
              "      <td>['mustread', 'every', 'southern', 'lady', 'fai...</td>\n",
              "      <td>mustread every southern lady failed love book ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40001</td>\n",
              "      <td>horrible watch napoleon want funny movie sucks...</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>['horrible', 'watch', 'napoleon', 'want', 'fun...</td>\n",
              "      <td>horrible watch napoleon want funny movie suck ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5026048-7feb-43ba-b4a8-bf9ea11f861e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d5026048-7feb-43ba-b4a8-bf9ea11f861e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d5026048-7feb-43ba-b4a8-bf9ea11f861e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import ast\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"ABSA_dataset.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"tokens\"] = df[\"tokens\"].apply(ast.literal_eval)  # Convert stored lists back to Python lists\n",
        "\n",
        "# Load SpaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def bio_tagging(tokens):\n",
        "    text = \" \".join(tokens)  # Convert token list to text\n",
        "    doc = nlp(text)\n",
        "\n",
        "    tags = [\"O\"] * len(tokens)  # Default all tokens as 'O'\n",
        "\n",
        "    for chunk in doc.noun_chunks:  # Detect noun phrases\n",
        "        chunk_tokens = chunk.text.split()\n",
        "        start_idx = -1\n",
        "\n",
        "        # Find start index of noun chunk in token list\n",
        "        for i in range(len(tokens) - len(chunk_tokens) + 1):\n",
        "            if tokens[i:i + len(chunk_tokens)] == chunk_tokens:\n",
        "                start_idx = i\n",
        "                break\n",
        "\n",
        "        # Assign BIO tags\n",
        "        if start_idx != -1:\n",
        "            tags[start_idx] = \"B-Aspect\"\n",
        "            for i in range(start_idx + 1, start_idx + len(chunk_tokens)):\n",
        "                tags[i] = \"I-Aspect\"\n",
        "\n",
        "    return tags\n",
        "\n",
        "# Apply BIO tagging function\n",
        "df[\"bio_tags\"] = df[\"tokens\"].apply(bio_tagging)\n",
        "\n",
        "# Save dataset with BIO tags\n",
        "df.to_csv(\"bio_tagged_dataset.csv\", index=False)\n",
        "\n",
        "print(\"✅ BIO-tagged dataset saved as 'bio_tagged_dataset.csv'!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "018-jxGQiSWw",
        "outputId": "99638c6b-d052-4ef2-8380-bd221d210f93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BIO-tagged dataset saved as 'bio_tagged_dataset.csv'!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load preprocessed dataset\n",
        "df = pd.read_csv(\"bio_tagged_dataset.csv\")\n",
        "\n",
        "# Convert token lists and BIO tag lists from string to actual lists\n",
        "import ast\n",
        "df[\"tokens\"] = df[\"tokens\"].apply(ast.literal_eval)\n",
        "df[\"bio_tags\"] = df[\"bio_tags\"].apply(ast.literal_eval)\n",
        "\n",
        "print(\"✅ Dataset loaded successfully!\")\n",
        "print(df.head())  # Display first few rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtJoJxBTiUUa",
        "outputId": "80de9f19-ac9a-4bde-d240-04d806a30078"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset loaded successfully!\n",
            "     No.                                        Review_Text  Sentiment_Label  \\\n",
            "0      1  one best game music soundtracks game didnt rea...                1   \n",
            "1  10001  best purchase ever bought exersaucer little gu...                1   \n",
            "2  20001  book slow weak one beststhank god slow weak bo...                0   \n",
            "3  30001  mustread every southern lady failed love book ...                1   \n",
            "4  40001  horrible watch napoleon want funny movie sucks...                0   \n",
            "\n",
            "   review_length                                             tokens  \\\n",
            "0             79  [one, best, game, music, soundtrack, game, did...   \n",
            "1             39  [best, purchase, ever, bought, exersaucer, lit...   \n",
            "2             20  [book, slow, weak, one, beststhank, god, slow,...   \n",
            "3             49  [mustread, every, southern, lady, failed, love...   \n",
            "4             32  [horrible, watch, napoleon, want, funny, movie...   \n",
            "\n",
            "                                      Cleaned_review  \\\n",
            "0  one best game music soundtrack game didnt real...   \n",
            "1  best purchase ever bought exersaucer little gu...   \n",
            "2  book slow weak one beststhank god slow weak bo...   \n",
            "3  mustread every southern lady failed love book ...   \n",
            "4  horrible watch napoleon want funny movie suck ...   \n",
            "\n",
            "                                            bio_tags  \n",
            "0  [B-Aspect, I-Aspect, I-Aspect, I-Aspect, I-Asp...  \n",
            "1  [B-Aspect, I-Aspect, O, O, B-Aspect, B-Aspect,...  \n",
            "2  [B-Aspect, O, B-Aspect, I-Aspect, I-Aspect, B-...  \n",
            "3  [O, B-Aspect, I-Aspect, I-Aspect, O, O, O, O, ...  \n",
            "4  [B-Aspect, I-Aspect, I-Aspect, O, B-Aspect, I-...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
        "\n",
        "# Label mapping for BIO tagging\n",
        "label_map = {\"O\": 0, \"B-Aspect\": 1, \"I-Aspect\": 2}\n",
        "\n",
        "def tokenize_and_align_labels(tokens, bio_tags):\n",
        "    tokenized_input = tokenizer(tokens, truncation=True, is_split_into_words=True, padding=\"longest\", add_special_tokens=True)\n",
        "    word_ids = tokenized_input.word_ids()\n",
        "\n",
        "    aligned_labels = []\n",
        "    prev_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id is None:\n",
        "            aligned_labels.append(-100)  # Ignore special tokens\n",
        "        elif word_id != prev_word:\n",
        "            aligned_labels.append(label_map[bio_tags[word_id]])  # Assign correct label\n",
        "        else:\n",
        "            aligned_labels.append(-100)  # Ignore subword parts\n",
        "        prev_word = word_id\n",
        "\n",
        "    tokenized_input[\"labels\"] = aligned_labels\n",
        "    return tokenized_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyOpUpSwiWRl",
        "outputId": "94c8052b-b8b5-4408-831c-3195b433472c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tokenized_inputs = []\n",
        "\n",
        "for tokens, bio_tags in tqdm(zip(df[\"tokens\"], df[\"bio_tags\"]), total=len(df)):\n",
        "    tokenized_input = tokenize_and_align_labels(tokens, bio_tags)\n",
        "    tokenized_inputs.append(tokenized_input)\n",
        "\n",
        "print(f\"✅ Tokenization completed for {len(tokenized_inputs)} samples.\")\n",
        "\n",
        "# Find max length from all tokenized inputs\n",
        "max_len = max(len(x[\"input_ids\"]) for x in tokenized_inputs)\n",
        "\n",
        "# Apply padding manually to ensure all tensors have the same size\n",
        "def pad_tensor(tensor, max_length, pad_value=0):\n",
        "    \"\"\"Pads tensor to max_length with pad_value.\"\"\"\n",
        "    padding_size = max_length - len(tensor)\n",
        "    return torch.cat([tensor, torch.full((padding_size,), pad_value, dtype=torch.long)])\n",
        "\n",
        "# Convert tokenized inputs to tensors and pad\n",
        "input_ids = torch.stack([pad_tensor(torch.tensor(x[\"input_ids\"]), max_len) for x in tokenized_inputs])\n",
        "attention_mask = torch.stack([pad_tensor(torch.tensor(x[\"attention_mask\"]), max_len) for x in tokenized_inputs])\n",
        "labels = torch.stack([pad_tensor(torch.tensor(x[\"labels\"]), max_len, pad_value=-100) for x in tokenized_inputs])  # ✅ Use -100 to ignore padding in loss calculation\n",
        "\n",
        "# Save dataset in PyTorch format\n",
        "torch.save({\n",
        "    \"input_ids\": input_ids,\n",
        "    \"attention_mask\": attention_mask,\n",
        "    \"labels\": labels\n",
        "}, \"bert_ner_dataset.pt\")\n",
        "\n",
        "print(f\"✅ Tokenized dataset saved successfully! (Padded to max length {max_len})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSvZ1pFRiYGe",
        "outputId": "0bc43381-eed8-474c-998c-3ab463c79887"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1310/1310 [00:02<00:00, 576.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tokenization completed for 1310 samples.\n",
            "✅ Tokenized dataset saved successfully! (Padded to max length 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "dataset = torch.load(\"bert_ner_dataset.pt\")\n",
        "\n",
        "# Label setup\n",
        "label_list = [\"O\", \"B-Aspect\", \"I-Aspect\"]\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "# Optional: sample for testing stability\n",
        "# dataset = {k: v[:300] for k, v in dataset.items()}  # Uncomment for quick tests"
      ],
      "metadata": {
        "id": "NuccgidUiaID"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class BERTNERDataset(Dataset):\n",
        "       def __init__(self, encodings):  # Changed _init_ to __init__\n",
        "           self.encodings = encodings\n",
        "       def __len__(self):\n",
        "           return len(self.encodings[\"input_ids\"])\n",
        "       def __getitem__(self, idx):\n",
        "           return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}"
      ],
      "metadata": {
        "id": "BvNxUOSqMbR8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/validation split\n",
        "train_size = int(0.8 * len(dataset[\"input_ids\"]))\n",
        "train_dataset = BERTNERDataset({k: v[:train_size] for k, v in dataset.items()})\n",
        "val_dataset = BERTNERDataset({k: v[train_size:] for k, v in dataset.items()})\n",
        "\n",
        "# Load tokenizer & model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioC5JRt9b0dy",
        "outputId": "3d3405e0-609a-4dea-d6e1-db1308830991"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_preds = [\n",
        "        [label_list[p] for (p, l) in zip(pred_row, label_row) if l != -100]\n",
        "        for pred_row, label_row in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(pred_row, label_row) if l != -100]\n",
        "        for pred_row, label_row in zip(predictions, labels)\n",
        "    ]\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_preds),\n",
        "        \"recall\": recall_score(true_labels, true_preds),\n",
        "        \"f1\": f1_score(true_labels, true_preds),\n",
        "    }"
      ],
      "metadata": {
        "id": "tj5YCSWpeDPq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration (Colab-optimized)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    logging_dir=\"./logs\",\n",
        "    fp16=torch.cuda.is_available(),  # Mixed precision if GPU available\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1LwJ2YQeFsV",
        "outputId": "2b354d08-3bd6-4466-fc1d-87df3acb117e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-d47e8af2e48e>:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save model/tokenizer\n",
        "model.save_pretrained(\"roberta_ate_model\")\n",
        "tokenizer.save_pretrained(\"roberta_tokenizer\")\n",
        "\n",
        "print(\"✅ Model training complete and saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "MYRxPx3Hilm7",
        "outputId": "1a8932c0-afe7-47b3-9caa-48b75873ef08"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-f19d8d194437>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='143' max='393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [143/393 2:05:13 < 3:42:00, 0.02 it/s, Epoch 1.08/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.589008</td>\n",
              "      <td>0.434575</td>\n",
              "      <td>0.480718</td>\n",
              "      <td>0.456484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-f19d8d194437>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='393' max='393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [393/393 5:48:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.589008</td>\n",
              "      <td>0.434575</td>\n",
              "      <td>0.480718</td>\n",
              "      <td>0.456484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.586800</td>\n",
              "      <td>0.457659</td>\n",
              "      <td>0.508188</td>\n",
              "      <td>0.481602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.576258</td>\n",
              "      <td>0.466311</td>\n",
              "      <td>0.508188</td>\n",
              "      <td>0.486350</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-f19d8d194437>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-13-f19d8d194437>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model training complete and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Zip and download ATE model\n",
        "!zip -r roberta_ate_model.zip roberta_ate_model\n",
        "files.download(\"roberta_ate_model.zip\")\n",
        "\n",
        "# Zip and download tokenizer\n",
        "!zip -r roberta_tokenizer.zip roberta_tokenizer\n",
        "files.download(\"roberta_tokenizer.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "nhvoyh5vYJQX",
        "outputId": "880245ee-ffd2-4ce6-c84a-abc6220d16b0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: roberta_ate_model/ (stored 0%)\n",
            "  adding: roberta_ate_model/config.json (deflated 51%)\n",
            "  adding: roberta_ate_model/model.safetensors (deflated 15%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5293be8b-2456-4734-b44d-19328a7950f6\", \"roberta_ate_model.zip\", 422231537)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: roberta_tokenizer/ (stored 0%)\n",
            "  adding: roberta_tokenizer/special_tokens_map.json (deflated 52%)\n",
            "  adding: roberta_tokenizer/tokenizer.json (deflated 82%)\n",
            "  adding: roberta_tokenizer/vocab.json (deflated 59%)\n",
            "  adding: roberta_tokenizer/tokenizer_config.json (deflated 75%)\n",
            "  adding: roberta_tokenizer/merges.txt (deflated 53%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ed978885-86a2-4ff6-9c9b-3b547ac63fa2\", \"roberta_tokenizer.zip\", 1189100)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}