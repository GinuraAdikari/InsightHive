{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "CRxB48iDiAF_",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:09:09.503888Z",
     "start_time": "2025-04-02T11:09:09.496770Z"
    }
   },
   "source": [
    "# Import Python built-in libraries\n",
    "import copy\n",
    "import pickle\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HGQ13SWwiHRl",
    "ExecuteTime": {
     "end_time": "2025-04-02T12:12:18.451391Z",
     "start_time": "2025-04-02T12:12:18.437073Z"
    }
   },
   "source": [
    "# Import pip libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Import torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "# Import PyG packages\n",
    "import torch_geometric as pyg\n",
    "import torch_geometric.data as pyg_data\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "import torch_sparse"
   ],
   "outputs": [],
   "execution_count": 130
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pn79u2Vc7Bi7",
    "outputId": "750d5825-42cc-42f2-9a1d-a64d13259797",
    "ExecuteTime": {
     "end_time": "2025-04-02T12:12:19.763453Z",
     "start_time": "2025-04-02T12:12:18.978805Z"
    }
   },
   "source": [
    "# Load and have a peek into the dataset\n",
    "events = pd.read_csv('events.csv')\n",
    "print(events.head())\n",
    "print(f'There are {len(events)} rows in the raw data.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       timestamp  visitorid event  itemid  transactionid\n",
      "0  1433221332117     257597  view  355908            NaN\n",
      "1  1433224214164     992329  view  248676            NaN\n",
      "2  1433221999827     111016  view  318965            NaN\n",
      "3  1433221955914     483717  view  253185            NaN\n",
      "4  1433221337106     951259  view  367447            NaN\n",
      "There are 2756101 rows in the raw data.\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "oD7SEiObBC8x",
    "outputId": "6218a809-1c60-4797-9c42-c6de8ffa93d6",
    "ExecuteTime": {
     "end_time": "2025-04-02T12:12:20.180243Z",
     "start_time": "2025-04-02T12:12:19.832985Z"
    }
   },
   "source": [
    "# Print the summarized statistics of the dataset\n",
    "events.describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          timestamp     visitorid        itemid  transactionid\n",
       "count  2.756101e+06  2.756101e+06  2.756101e+06   22457.000000\n",
       "mean   1.436424e+12  7.019229e+05  2.349225e+05    8826.497796\n",
       "std    3.366312e+09  4.056875e+05  1.341954e+05    5098.996290\n",
       "min    1.430622e+12  0.000000e+00  3.000000e+00       0.000000\n",
       "25%    1.433478e+12  3.505660e+05  1.181200e+05    4411.000000\n",
       "50%    1.436453e+12  7.020600e+05  2.360670e+05    8813.000000\n",
       "75%    1.439225e+12  1.053437e+06  3.507150e+05   13224.000000\n",
       "max    1.442545e+12  1.407579e+06  4.668670e+05   17671.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.756101e+06</td>\n",
       "      <td>2.756101e+06</td>\n",
       "      <td>2.756101e+06</td>\n",
       "      <td>22457.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.436424e+12</td>\n",
       "      <td>7.019229e+05</td>\n",
       "      <td>2.349225e+05</td>\n",
       "      <td>8826.497796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.366312e+09</td>\n",
       "      <td>4.056875e+05</td>\n",
       "      <td>1.341954e+05</td>\n",
       "      <td>5098.996290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.430622e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.433478e+12</td>\n",
       "      <td>3.505660e+05</td>\n",
       "      <td>1.181200e+05</td>\n",
       "      <td>4411.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.436453e+12</td>\n",
       "      <td>7.020600e+05</td>\n",
       "      <td>2.360670e+05</td>\n",
       "      <td>8813.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.439225e+12</td>\n",
       "      <td>1.053437e+06</td>\n",
       "      <td>3.507150e+05</td>\n",
       "      <td>13224.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.442545e+12</td>\n",
       "      <td>1.407579e+06</td>\n",
       "      <td>4.668670e+05</td>\n",
       "      <td>17671.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 132
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLaXTMI3TqYe",
    "outputId": "23fd452c-2ec8-483c-ddc5-f4a62ee17638",
    "ExecuteTime": {
     "end_time": "2025-04-02T12:12:20.534456Z",
     "start_time": "2025-04-02T12:12:20.416345Z"
    }
   },
   "source": [
    "# Check the maximum unique id of items.\n",
    "# \n",
    "max(events['itemid'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466867"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 133
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36yeTCZ6-O6_",
    "outputId": "27afc431-2bf5-458d-953f-9029c796ca88",
    "ExecuteTime": {
     "end_time": "2025-04-02T12:12:20.979081Z",
     "start_time": "2025-04-02T12:12:20.839956Z"
    }
   },
   "source": [
    "# Filter only the 'view' events.\n",
    "view_events = events[events['event'] == 'view']\n",
    "print(f'There are {len(view_events)} `view` events in the raw data.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2664312 `view` events in the raw data.\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "72rxoxUqBT3J",
    "ExecuteTime": {
     "end_time": "2025-04-02T12:12:21.569117Z",
     "start_time": "2025-04-02T12:12:21.249772Z"
    }
   },
   "source": [
    "# Filter out visitors with single clicks.\n",
    "visits_per_visitor = view_events['visitorid'].value_counts(dropna=False)\n",
    "visits_per_visitor.head()\n",
    "\n",
    "filtered_visits_of_visitors = visits_per_visitor[\n",
    "    visits_per_visitor > 1\n",
    "].index\n",
    "view_events = view_events[\n",
    "    view_events['visitorid'].isin(filtered_visits_of_visitors)]"
   ],
   "outputs": [],
   "execution_count": 135
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQDjX6S1D-nH",
    "outputId": "5d04fea2-11e3-491b-9898-abdbb29f7982",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-02T12:12:21.971967Z"
    }
   },
   "source": [
    "# Let's group events and their timing data.\n",
    "visits_data = {}\n",
    "for _, row in enumerate(tqdm(view_events.iterrows())):\n",
    "    timestamp, visitorid, event, itemid, transactionid = row[1].values\n",
    "\n",
    "    if visitorid not in visits_data:\n",
    "        visits_data[visitorid] = {'itemids': [], 'timestamps': []}\n",
    "    visits_data[visitorid]['itemids'].append(itemid)\n",
    "    visits_data[visitorid]['timestamps'].append(timestamp)\n",
    "\n",
    "print()\n",
    "print(f'There are {len(visits_data)} visitors left.')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44714it [00:01, 55863.70it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YdCvwXpIpN6",
    "outputId": "d0c0f98c-1147-46d0-81fb-333c1954c794",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:09:50.845748Z",
     "start_time": "2025-04-02T11:09:49.713705Z"
    }
   },
   "source": [
    "# We will separate sessions by 1 hr.\n",
    "session_gap = 1 * 3600 * 1000\n",
    "\n",
    "# Let's group events from visitors into sessions.\n",
    "sessions_of_visitors = {}\n",
    "for visitorid, visitor_dict in visits_data.items():\n",
    "    sessions = [[]]\n",
    "    sorted_events = sorted(zip(visitor_dict['timestamps'],\n",
    "                               visitor_dict['itemids']))\n",
    "    for i in range(len(sorted_events) - 1):\n",
    "        sessions[-1].append(sorted_events[i][1])\n",
    "        if (sorted_events[i+1][0] - sorted_events[i][0]) > session_gap:\n",
    "            sessions.append([])\n",
    "    sessions[-1].append(sorted_events[len(sorted_events) - 1][1])\n",
    "    sessions_of_visitors[visitorid] = sessions\n",
    "\n",
    "print()\n",
    "print(f'There are {len(sessions_of_visitors)} sessions.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 396449 sessions.\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "80Shzu4P-s_9",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:09:51.804099Z",
     "start_time": "2025-04-02T11:09:51.616558Z"
    }
   },
   "source": [
    "# Adjsut sampling rate ([0, 1]) to generate smaller datasets.\n",
    "# Setting `sampling_rate` to 1 will lead to a full dataset split.\n",
    "sampling_rate = 0.5\n",
    "\n",
    "# We use random seed for reproducibility.\n",
    "seed = 42\n",
    "all_visitors = list(sessions_of_visitors.keys())\n",
    "random.Random(seed).shuffle(all_visitors)\n",
    "\n",
    "num_train = int(len(all_visitors) * 0.8 * sampling_rate)\n",
    "num_val = int(len(all_visitors) * 0.1 * sampling_rate)\n",
    "num_test = int(len(all_visitors) * 0.1 * sampling_rate)\n",
    "\n",
    "train_visitors = all_visitors[:num_train]\n",
    "val_visitors = all_visitors[num_train : num_train+num_val]\n",
    "test_visitors = all_visitors[num_train+num_val:num_train+num_val+num_test]"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vOt9UVcesil7",
    "ExecuteTime": {
     "end_time": "2025-04-02T12:11:59.644691Z",
     "start_time": "2025-04-02T12:11:59.637651Z"
    }
   },
   "source": [
    "def extract_subsessions(sessions):\n",
    "    all_sessions = []\n",
    "    for session in sessions:\n",
    "        for i in range(1, len(session)):\n",
    "            all_sessions.append(session[:i+1])\n",
    "    return all_sessions"
   ],
   "outputs": [],
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOTFciFbAHDl",
    "outputId": "cd7a9dd2-3944-409c-97cf-28ff20555ddd",
    "ExecuteTime": {
     "end_time": "2025-04-02T12:12:03.360107Z",
     "start_time": "2025-04-02T12:12:03.312930Z"
    }
   },
   "source": [
    "# Check the number of visitors in each split\n",
    "print(f'train, val, and test visitors: {len(train_visitors), len(val_visitors), len(test_visitors)}')\n",
    "\n",
    "train_sessions, val_sessions, test_sessions = [], [], []\n",
    "for visitor in train_visitors:\n",
    "    train_sessions.extend(extract_subsessions(sessions_of_visitors[visitor]))\n",
    "for visitor in val_visitors:\n",
    "    val_sessions.extend(extract_subsessions(sessions_of_visitors[visitor]))\n",
    "for visitor in test_visitors:\n",
    "    test_sessions.extend(extract_subsessions(sessions_of_visitors[visitor]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, val, and test visitors: (158579, 19822, 19822)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sessions_of_visitors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[129], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m train_sessions, val_sessions, test_sessions \u001B[38;5;241m=\u001B[39m [], [], []\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m visitor \u001B[38;5;129;01min\u001B[39;00m train_visitors:\n\u001B[1;32m----> 6\u001B[0m     train_sessions\u001B[38;5;241m.\u001B[39mextend(extract_subsessions(\u001B[43msessions_of_visitors\u001B[49m[visitor]))\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m visitor \u001B[38;5;129;01min\u001B[39;00m val_visitors:\n\u001B[0;32m      8\u001B[0m     val_sessions\u001B[38;5;241m.\u001B[39mextend(extract_subsessions(sessions_of_visitors[visitor]))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sessions_of_visitors' is not defined"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHNN9AV55P-v",
    "outputId": "538dbb6d-b5c2-4498-c502-9a5e95bf76e5",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:09:57.325340Z",
     "start_time": "2025-04-02T11:09:57.068200Z"
    }
   },
   "source": [
    "# Check the number of (sub)sessions in each split\n",
    "print(f'train, val, and test sessions: {len(train_sessions), len(val_sessions), len(test_sessions)}')\n",
    "\n",
    "# Save the processed files.\n",
    "with open('raw/train.txt', 'wb') as f:\n",
    "    pickle.dump(train_sessions, f)\n",
    "with open('raw/val.txt', 'wb') as f:\n",
    "    pickle.dump(val_sessions, f)\n",
    "with open('raw/test.txt', 'wb') as f:\n",
    "    pickle.dump(test_sessions, f)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, val, and test sessions: (373755, 49563, 52584)\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "82KWESLd-1gV",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:09:58.297450Z",
     "start_time": "2025-04-02T11:09:58.284074Z"
    }
   },
   "source": [
    "class GraphDataset(pyg_data.InMemoryDataset):\n",
    "    def __init__(self, root, file_name, transform=None, pre_transform=None):\n",
    "        self.file_name = file_name\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [f'{self.file_name}.txt']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'{self.file_name}.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        raw_data_file = f'{self.raw_dir}/{self.raw_file_names[0]}'\n",
    "        with open(raw_data_file, 'rb') as f:\n",
    "            sessions = pickle.load(f)\n",
    "        data_list = []\n",
    "\n",
    "        for session in sessions:\n",
    "            session, y = session[:-1], session[-1]\n",
    "            codes, uniques = pd.factorize(session)\n",
    "            senders, receivers = codes[:-1], codes[1:]\n",
    "\n",
    "            # Build Data instance\n",
    "            edge_index = torch.tensor(np.array([senders, receivers]))\n",
    "            x = torch.tensor(uniques, dtype=torch.long).unsqueeze(1)\n",
    "            y = torch.tensor([y], dtype=torch.long)\n",
    "            data_list.append(pyg_data.Data(x=x, edge_index=edge_index, y=y))\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ],
   "outputs": [],
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7K3HWrcwqp0Z",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:09:59.075403Z",
     "start_time": "2025-04-02T11:09:59.062295Z"
    }
   },
   "source": [
    "class GatedSessionGraphConv(pyg.nn.conv.MessagePassing):\n",
    "    def __init__(self, out_channels, aggr: str = 'add', **kwargs):\n",
    "        super().__init__(aggr=aggr, **kwargs)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.gru = torch.nn.GRUCell(out_channels, out_channels, bias=False)\n",
    "\n",
    "    # forward() function will contain following two things\n",
    "              ## 1. Message propagation to create and use an adjacency matrix (self.propagate).\n",
    "              ## 2. The GRU cell (self.gru)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        m = self.propagate(edge_index, x=x, size=None)\n",
    "        x = self.gru(m, x)\n",
    "        return x\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return x_j\n",
    "\n",
    "    # the `message_and_aggregate()` function will simply multiply our adjacency matrix with the node embeddings\n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        return matmul(adj_t, x, reduce=self.aggr)"
   ],
   "outputs": [],
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TNsAHq8PCN3k",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:09:59.792428Z",
     "start_time": "2025-04-02T11:09:59.778425Z"
    }
   },
   "source": [
    "class SRGNN(nn.Module):\n",
    "    def __init__(self, hidden_size, n_items):\n",
    "        super(SRGNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_items = n_items\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_items, self.hidden_size)\n",
    "        # use message-passing class inside the SRGNN class\n",
    "        self.gated = GatedSessionGraphConv(self.hidden_size)\n",
    "\n",
    "        self.q = nn.Linear(self.hidden_size, 1)\n",
    "        self.W_1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.W_2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.W_3 = nn.Linear(2 * self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_map = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # (0) \n",
    "        embedding = self.embedding(x).squeeze()\n",
    "\n",
    "        # (1)-(5) \n",
    "        v_i = self.gated(embedding, edge_index)\n",
    "\n",
    "        # Divide nodes by session\n",
    "        sections = list(torch.bincount(batch_map).cpu())\n",
    "        v_i_split = torch.split(v_i, sections)\n",
    "\n",
    "        v_n, v_n_repeat = [], []\n",
    "        for session in v_i_split:\n",
    "            v_n.append(session[-1])\n",
    "            v_n_repeat.append(\n",
    "                session[-1].view(1, -1).repeat(session.shape[0], 1))\n",
    "        v_n, v_n_repeat = torch.stack(v_n), torch.cat(v_n_repeat, dim=0)\n",
    "\n",
    "        q1 = self.W_1(v_n_repeat)\n",
    "        q2 = self.W_2(v_i)\n",
    "\n",
    "        # (6) creating global Embedding : The global embedding is the weighted average of the embeddings of the items in the session. \n",
    "        alpha = self.q(F.sigmoid(q1 + q2))\n",
    "        s_g_split = torch.split(alpha * v_i, sections)\n",
    "\n",
    "        s_g = []\n",
    "        for session in s_g_split:\n",
    "            s_g_session = torch.sum(session, dim=0)\n",
    "            s_g.append(s_g_session)\n",
    "        s_g = torch.stack(s_g)\n",
    "\n",
    "        # (7) The final hybrid embedding of a session is created by first concatenating the local and global embeddings\n",
    "        s_l = v_n # local Embedding\n",
    "        s_h = self.W_3(torch.cat([s_l, s_g], dim=-1))\n",
    "\n",
    "        # (8) The final scores of each item are computed by computing the cosine similarity between the session embedding (1 x d) \n",
    "        # and the embeddings of all 466867 unique items (466867 x d)\n",
    "        z = torch.mm(self.embedding.weight, s_h.T).T\n",
    "        return z"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T11:10:01.373967Z",
     "start_time": "2025-04-02T11:10:01.367166Z"
    }
   },
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.enabled)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T11:10:02.269870Z",
     "start_time": "2025-04-02T11:10:02.262801Z"
    }
   },
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\ML viva\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T11:10:03.167199Z",
     "start_time": "2025-04-02T11:10:03.151847Z"
    }
   },
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU Name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5gMZqEW81HYj",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:10:04.208813Z",
     "start_time": "2025-04-02T11:10:04.200463Z"
    }
   },
   "source": [
    "# Define the hyperparameters.\n",
    "# Code taken from 2021 Fall CS224W Colab assignments.\n",
    "args = {\n",
    "    'batch_size': 128,\n",
    "    'hidden_dim': 64,\n",
    "    'epochs': 10,\n",
    "    'l2_penalty': 0.0001,\n",
    "    'weight_decay': 0.1,\n",
    "    'step': 5,\n",
    "    'lr': 0.001,\n",
    "    'num_items': 466868}\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d): \n",
    "        self.__dict__ = d\n",
    "\n",
    "args = objectview(args)"
   ],
   "outputs": [],
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T17:33:04.862546Z",
     "start_time": "2025-04-01T17:33:04.846364Z"
    }
   },
   "source": [
    "print(\"w\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8iVoTPoJAKkU",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:10:07.179770Z",
     "start_time": "2025-04-02T11:10:07.167466Z"
    }
   },
   "source": [
    "def train(args):\n",
    "    # Prepare data pipeline\n",
    "    train_dataset = GraphDataset('./', 'train')\n",
    "    train_loader = pyg_data.DataLoader(train_dataset,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       shuffle=False,\n",
    "                                       drop_last=True)\n",
    "    val_dataset = GraphDataset('./', 'val')\n",
    "    val_loader = pyg_data.DataLoader(val_dataset,\n",
    "                                     batch_size=args.batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     drop_last=True)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # Build model\n",
    "    model = SRGNN(args.hidden_dim, args.num_items).to(device)\n",
    "\n",
    "    # Get training components\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.l2_penalty)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step,\n",
    "                                          gamma=args.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    top_k_accs = []\n",
    "\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for _, batch in enumerate(tqdm(train_loader)):\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            loss = criterion(pred, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "          test_acc, top_k_acc = test(val_loader, model, is_validation=True)\n",
    "          print(test_acc)\n",
    "          test_accs.append(test_acc)\n",
    "          top_k_accs.append(top_k_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "  \n",
    "    return test_accs, top_k_accs, losses, best_model, best_acc, val_loader"
   ],
   "outputs": [],
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T19:07:31.750123Z",
     "start_time": "2025-03-28T19:07:31.742101Z"
    },
    "id": "UTiJJ1pm0ks9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▊                                                                                                                     | 11/747 [00:02<05:31,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▎                                                                                                                   | 21/747 [00:05<05:15,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▉                                                                                                                  | 31/747 [00:08<04:56,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▌                                                                                                                | 41/747 [00:11<05:39,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████████                                                                                                               | 51/747 [00:13<04:57,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████████▋                                                                                                             | 61/747 [00:16<04:45,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▎                                                                                                           | 71/747 [00:19<04:56,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████████▉                                                                                                          | 81/747 [00:21<04:36,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████████▍                                                                                                        | 91/747 [00:24<04:53,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████████▉                                                                                                      | 101/747 [00:27<04:32,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████▌                                                                                                    | 111/747 [00:29<04:29,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 110 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████████                                                                                                   | 121/747 [00:32<04:23,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 120 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████████████████▋                                                                                                 | 131/747 [00:35<04:23,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 130 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████████████████████▎                                                                                               | 141/747 [00:37<03:43,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 140 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▊                                                                                              | 151/747 [00:40<04:45,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 150 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████████████▍                                                                                            | 161/747 [00:43<03:39,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 160 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|███████████████████████████                                                                                           | 171/747 [00:46<04:09,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 170 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████████████▌                                                                                         | 181/747 [00:48<04:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 180 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████████████████████▏                                                                                       | 191/747 [00:51<03:36,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 190 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████████████████████████▊                                                                                      | 201/747 [00:54<03:44,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████████▎                                                                                    | 211/747 [00:56<03:45,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 210 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████▉                                                                                   | 221/747 [00:59<03:48,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 220 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████████████████▍                                                                                 | 231/747 [01:01<03:13,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 230 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████████████████                                                                                | 241/747 [01:04<03:44,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 240 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████████████████▋                                                                              | 251/747 [01:07<03:16,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████████████████▍                                                                            | 262/747 [01:10<02:52,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 260 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████████████▊                                                                           | 271/747 [01:12<03:11,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 270 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████████████████▍                                                                         | 281/747 [01:15<03:28,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 280 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|█████████████████████████████████████████████▉                                                                        | 291/747 [01:18<03:17,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 290 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████▌                                                                      | 301/747 [01:20<03:09,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 300 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████████████████████▏                                                                    | 311/747 [01:23<02:46,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 310 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████████████████████████████▋                                                                   | 321/747 [01:26<03:13,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 320 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████████████████████▎                                                                 | 331/747 [01:29<02:56,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 330 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████████████████████████████▊                                                                | 341/747 [01:31<02:37,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 340 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████████████████████████████████████████▍                                                              | 351/747 [01:34<02:29,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 350 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████████                                                             | 361/747 [01:36<02:38,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 360 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████▌                                                           | 371/747 [01:39<02:36,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 370 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████████████████████████████████████████████▏                                                         | 381/747 [01:41<02:28,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 380 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████████████████████████▊                                                        | 391/747 [01:44<02:28,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 390 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████████████████████████▎                                                      | 401/747 [01:47<02:09,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████████████████████████▉                                                     | 411/747 [01:49<02:01,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 410 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████▌                                                   | 421/747 [01:52<02:18,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 420 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████████████████████████                                                  | 431/747 [01:55<02:07,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 430 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████████████████████████████████████████████████▋                                                | 441/747 [01:57<02:12,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 440 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████▏                                              | 451/747 [02:00<02:03,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 450 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████████████████████████████████████████████████▊                                             | 461/747 [02:02<01:58,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 460 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████████████████████████████▍                                           | 471/747 [02:05<01:54,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 470 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████████████████████████████▉                                          | 481/747 [02:08<01:54,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 480 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████████████████████████████████████████████████████████▌                                        | 491/747 [02:10<01:32,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 490 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████████████████████████████▏                                      | 501/747 [02:13<01:39,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████████████████████████████▋                                     | 511/747 [02:16<01:43,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 510 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████▎                                   | 521/747 [02:19<01:34,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 520 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████████████████████████████████████████████████████▋                                  | 530/747 [02:21<02:10,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 530 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████████████████████████████████▍                                | 541/747 [02:24<01:26,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 540 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████████████████████████████████████████████████████████████████████████                               | 551/747 [02:27<01:27,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 550 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████████████████████████▌                             | 561/747 [02:29<01:12,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 560 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████████████████████████████▏                           | 571/747 [02:32<01:25,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 570 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████▊                          | 581/747 [02:35<01:15,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 580 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████▎                        | 591/747 [02:38<01:11,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 590 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████▊                       | 600/747 [02:41<01:07,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 600 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████████████████████████████████████████████▌                     | 611/747 [02:43<00:46,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 610 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████                    | 621/747 [02:46<00:52,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 620 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                  | 631/747 [02:49<00:49,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 630 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 641/747 [02:51<00:46,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 640 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 650/747 [02:54<00:53,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 650 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 661/747 [02:57<00:36,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 660 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 670/747 [02:59<00:45,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 670 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 681/747 [03:02<00:26,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 680 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 691/747 [03:05<00:23,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 690 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋       | 701/747 [03:07<00:17,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 700 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎     | 711/747 [03:10<00:15,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 710 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 721/747 [03:13<00:11,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 720 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 731/747 [03:15<00:06,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 730 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 741/747 [03:18<00:02,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 740 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 747/747 [03:19<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_rate': 0.250892458723784, 'hit_rate@k': 0.5554439982150825, 'precision@k': 0.6428571428571429, 'recall@k': 0.6428571428571429, 'mrr': 0.38575762477770986, 'ndcg@k': 0.4410254088639926, 'diversity': 0.41785714285714287, 'novelty': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def precision_at_k(predictions, labels, k):\n",
    "    top_k_preds = predictions.argsort(axis=1)[:, -k:]\n",
    "    hits = (top_k_preds == labels[:, None]).sum(axis=1)\n",
    "    return hits.mean()\n",
    "\n",
    "def recall_at_k(predictions, labels, k):\n",
    "    top_k_preds = predictions.argsort(axis=1)[:, -k:]\n",
    "    hits = (top_k_preds == labels[:, None]).sum(axis=1)\n",
    "    return hits.mean()\n",
    "\n",
    "def mean_reciprocal_rank(predictions, labels):\n",
    "    recip_ranks = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "        pred = predictions[i]\n",
    "        rank = np.where(pred.argsort()[::-1] == labels[i])[0][0]\n",
    "        recip_ranks.append(1.0 / (rank + 1))\n",
    "    return np.mean(recip_ranks)\n",
    "\n",
    "def ndcg_at_k(predictions, labels, k):\n",
    "    def dcg(scores):\n",
    "        return np.sum(scores / np.log2(np.arange(2, scores.size + 2)))\n",
    "\n",
    "    ndcg_scores = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "        pred = predictions[i]\n",
    "        true_relevance = np.zeros(pred.shape)\n",
    "        true_relevance[labels[i]] = 1\n",
    "        ranked_relevance = true_relevance[pred.argsort()[::-1]]\n",
    "        ideal_relevance = np.sort(true_relevance)[::-1]\n",
    "        ndcg_scores.append(dcg(ranked_relevance[:k]) / dcg(ideal_relevance[:k]))\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "def diversity(predictions, k):\n",
    "    top_k_preds = predictions.argsort(axis=1)[:, -k:]\n",
    "    unique_items = np.unique(top_k_preds)\n",
    "    return len(unique_items) / (predictions.shape[0] * k)\n",
    "\n",
    "def novelty(predictions, k, item_popularity):\n",
    "    top_k_preds = predictions.argsort(axis=1)[:, -k:]\n",
    "    novel_items = (item_popularity[top_k_preds] == 0).sum()\n",
    "    return novel_items / (predictions.shape[0] * k)\n",
    "\n",
    "def save_checkpoint(epoch, data, filename='checkpoint.pth.tar'):\n",
    "    torch.save({'epoch': epoch, 'data': data}, filename)\n",
    "\n",
    "def test(loader, test_model, is_validation=False, save_model_preds=False, item_popularity=None, checkpoint_interval=10):\n",
    "    test_model.eval()\n",
    "\n",
    "    # Define K for metrics.\n",
    "    k = 10\n",
    "    correct = 0\n",
    "    top_k_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    batch_count = 0\n",
    "\n",
    "    for idx, data in enumerate(tqdm(loader)):\n",
    "        try:\n",
    "            data = data.to('cuda')\n",
    "            with torch.no_grad():\n",
    "                score = test_model(data)\n",
    "                pred = score.max(dim=1)[1]\n",
    "                label = data.y\n",
    "\n",
    "            if save_model_preds:\n",
    "                all_preds.extend(pred.view(-1).cpu().detach().numpy())\n",
    "                all_labels.extend(label.view(-1).cpu().detach().numpy())\n",
    "                all_scores.extend(score.cpu().detach().numpy())\n",
    "\n",
    "            correct += pred.eq(label).sum().item()\n",
    "            total_samples += label.size(0)\n",
    "\n",
    "            if not is_validation:\n",
    "                score = score.cpu().detach().numpy()\n",
    "                for row in range(pred.size(0)):\n",
    "                    top_k_pred = np.argpartition(score[row], -k)[-k:]\n",
    "                    if label[row].item() in top_k_pred:\n",
    "                        top_k_correct += 1\n",
    "\n",
    "            # Save checkpoint\n",
    "            if save_model_preds and (idx + 1) % checkpoint_interval == 0:\n",
    "                save_checkpoint(idx + 1, {\n",
    "                    'pred': all_preds,\n",
    "                    'label': all_labels,\n",
    "                    'score': all_scores\n",
    "                })\n",
    "                # Clear intermediate results to free up memory\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                all_scores = []\n",
    "\n",
    "            # Clear CUDA cache to free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            batch_count += 1\n",
    "            if batch_count % 10 == 0:\n",
    "                print(f\"Processed {batch_count} batches\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if save_model_preds:\n",
    "        data = {\n",
    "            'pred': all_preds,\n",
    "            'label': all_labels,\n",
    "            'score': all_scores\n",
    "        }\n",
    "        df = pd.DataFrame(data=data)\n",
    "        df.to_csv('pred.csv', sep=',', index=False)\n",
    "\n",
    "    if not is_validation:\n",
    "        all_scores = np.array(all_scores)\n",
    "        all_labels = np.array(all_labels)\n",
    "        precision_k = precision_at_k(all_scores, all_labels, k)\n",
    "        recall_k = recall_at_k(all_scores, all_labels, k)\n",
    "        mrr = mean_reciprocal_rank(all_scores, all_labels)\n",
    "        ndcg_k = ndcg_at_k(all_scores, all_labels, k)\n",
    "        diversity_score = diversity(all_scores, k)\n",
    "        novelty_score = novelty(all_scores, k, item_popularity) if item_popularity is not None else 0\n",
    "\n",
    "        return {\n",
    "            'hit_rate': correct / total_samples,\n",
    "            'hit_rate@k': top_k_correct / total_samples,\n",
    "            'precision@k': precision_k,\n",
    "            'recall@k': recall_k,\n",
    "            'mrr': mrr,\n",
    "            'ndcg@k': ndcg_k,\n",
    "            'diversity': diversity_score,\n",
    "            'novelty': novelty_score\n",
    "        }\n",
    "    else:\n",
    "        return correct / total_samples, 0\n",
    "\n",
    "# Run test for our best model to save the predictions!\n",
    "test_dataset = GraphDataset('./', 'test')\n",
    "test_loader = pyg_data.DataLoader(test_dataset,\n",
    "                                  batch_size=args.batch_size // 10,  # Further reduce batch size\n",
    "                                  shuffle=False,\n",
    "                                  drop_last=True)\n",
    "\n",
    "# Assuming item_popularity is a numpy array with the popularity of each item\n",
    "item_popularity = np.zeros((args.num_items,))\n",
    "\n",
    "metrics = test(test_loader, best_model, is_validation=False, save_model_preds=True, item_popularity=item_popularity)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T11:10:25.000086Z",
     "start_time": "2025-04-02T11:10:24.986075Z"
    }
   },
   "source": [
    "def test(loader, test_model, is_validation=False, save_model_preds=False):\n",
    "    test_model.eval()\n",
    "\n",
    "    # Define K for Hit@K metrics.\n",
    "    k = 10\n",
    "    correct = 0\n",
    "    top_k_correct = 0\n",
    "\n",
    "    for _, data in enumerate(tqdm(loader)):\n",
    "        data.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            score = test_model(data)\n",
    "            pred = score.max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        if save_model_preds:\n",
    "          data = {}\n",
    "          data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "          data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "\n",
    "          df = pd.DataFrame(data=data)\n",
    "          # Save locally as csv\n",
    "          df.to_csv('pred.csv', sep=',', index=False)\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "        # We calculate Hit@K accuracy only at test time.\n",
    "        if not is_validation:\n",
    "            score = score.cpu().detach().numpy()\n",
    "            for row in range(pred.size(0)):\n",
    "                top_k_pred = np.argpartition(score[row], -k)[-k:]\n",
    "                if label[row].item() in top_k_pred:\n",
    "                    top_k_correct += 1\n",
    "    \n",
    "    if not is_validation:\n",
    "        return correct / len(loader), top_k_correct / len(loader)\n",
    "    else:\n",
    "        return correct / len(loader), 0"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T12:39:22.976778800Z",
     "start_time": "2025-03-28T19:07:40.152640Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pdNUqKJl1r35",
    "outputId": "e2e8b168-4ad1-4830-cb65-c3b3ff3d3cba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\ML viva\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [04:23<00:00, 11.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:09<00:00, 40.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.958656330749355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [04:02<00:00, 12.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:07<00:00, 54.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.62532299741602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [04:03<00:00, 11.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:07<00:00, 53.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.167958656330748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [04:01<00:00, 12.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:07<00:00, 54.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.046511627906977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [04:01<00:00, 12.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:06<00:00, 57.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.705426356589147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [03:57<00:00, 12.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:06<00:00, 57.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.839793281653748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [03:58<00:00, 12.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:06<00:00, 56.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.20671834625323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [04:00<00:00, 12.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:07<00:00, 55.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.51421188630491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [03:59<00:00, 12.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:06<00:00, 56.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.785529715762273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2919/2919 [04:00<00:00, 12.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:06<00:00, 58.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.987080103359173\n",
      "[13.958656330749355, 13.62532299741602, 15.167958656330748, 16.046511627906977, 16.705426356589147, 18.839793281653748, 19.20671834625323, 19.51421188630491, 19.785529715762273, 19.987080103359173] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Maximum test set accuracy: 19.987080103359173\n",
      "Minimum loss: 5.800257827177636\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_accs, top_k_accs, losses, best_model, best_acc, test_loader = train(args) \n",
    "\n",
    "print(test_accs, top_k_accs)\n",
    "print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "# plt.title(dataset.name)\n",
    "plt.plot(losses, label=\"training loss\" + \" - \")\n",
    "plt.plot(test_accs, label=\"test accuracy\" + \" - \")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfUleigO6Xy1"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiiii0\n"
     ]
    }
   ],
   "source": [
    "print(\"hiiii0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9rTB4_lI1Ocg"
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "torch.save(best_model.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoK2gVD7HVep",
    "outputId": "0ba517f1-5d52-4ef2-ce23-201fe28454f3",
    "ExecuteTime": {
     "end_time": "2025-04-02T11:12:07.122405Z",
     "start_time": "2025-04-02T11:10:40.972995Z"
    }
   },
   "source": [
    "# Run test for our best model to save the predictions!\n",
    "test_dataset = GraphDataset('./', 'test')\n",
    "test_loader = pyg_data.DataLoader(test_dataset,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  drop_last=True)\n",
    "\n",
    "test(test_loader, best_model, is_validation=False, save_model_preds=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\ML viva\\.venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "100%|██████████| 70/70 [01:26<00:00,  1.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32.128571428571426, 71.11428571428571)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T11:10:37.750518Z",
     "start_time": "2025-04-02T11:10:37.361664Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "best_model = model = SRGNN(args.hidden_dim, args.num_items).to(device)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "best_model.load_state_dict(torch.load('model', map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "# If you're using CUDA\n",
    "#best_model.to('cuda')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SRGNN(\n",
       "  (embedding): Embedding(466868, 64)\n",
       "  (gated): GatedSessionGraphConv()\n",
       "  (q): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (W_1): Linear(in_features=64, out_features=64, bias=False)\n",
       "  (W_2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (W_3): Linear(in_features=128, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T16:01:55.013830Z",
     "start_time": "2025-04-02T16:01:48.462596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the preprocessed data\n",
    "with open('raw/train.txt', 'rb') as f:\n",
    "    train_sessions = pickle.load(f)\n",
    "with open('raw/val.txt', 'rb') as f:\n",
    "    val_sessions = pickle.load(f)\n",
    "with open('raw/test.txt', 'rb') as f:\n",
    "    test_sessions = pickle.load(f)\n",
    "\n",
    "# Combine all sessions\n",
    "all_sessions = train_sessions + val_sessions + test_sessions\n",
    "\n",
    "def generate_recommendations(visitor_id, sessions, model, top_k=10):\n",
    "    # Filter sessions for the given visitor ID\n",
    "    user_sessions = [session for session in sessions if session[0] == visitor_id]\n",
    "    if not user_sessions:\n",
    "        return \"No sessions found for the given visitor ID.\"\n",
    "\n",
    "    # Generate recommendations using the model\n",
    "    recommendations = []\n",
    "    for session in user_sessions:\n",
    "        if len(session) < 2:\n",
    "            continue  # Ensure session has enough elements\n",
    "        session_data = np.array(session[1:])  # Convert to NumPy array for factorization\n",
    "\n",
    "        # Build Data instance\n",
    "        codes, uniques = pd.factorize(session_data)\n",
    "        senders, receivers = codes[:-1], codes[1:]\n",
    "        if len(senders) == 0 or len(receivers) == 0:\n",
    "            continue  # Skip if there are no edges\n",
    "\n",
    "        edge_index = torch.tensor(np.array([senders, receivers]), dtype=torch.long).to(device)\n",
    "        x = torch.tensor(codes, dtype=torch.long).unsqueeze(1).to(device)\n",
    "        batch = torch.zeros(len(codes), dtype=torch.long).to(device)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, batch=batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            if output.numel() < top_k:\n",
    "                top_k = output.numel()  # Adjust top_k if not enough predictions\n",
    "            top_items = torch.topk(output.squeeze(), top_k).indices.tolist()\n",
    "            recommendations.extend(top_items)\n",
    "\n",
    "    # Flatten and get unique recommendations\n",
    "    unique_recommendations = list(set(recommendations))\n",
    "    return unique_recommendations[:top_k]\n",
    "\n",
    "# Example usage\n",
    "visitor_id = 1234\n",
    "recommendations = generate_recommendations(visitor_id, all_sessions, best_model, top_k=5)\n",
    "print(\"Top 10 Recommended Items:\", recommendations)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 58\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[0;32m     57\u001B[0m visitor_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1234\u001B[39m\n\u001B[1;32m---> 58\u001B[0m recommendations \u001B[38;5;241m=\u001B[39m generate_recommendations(visitor_id, all_sessions, \u001B[43mbest_model\u001B[49m, top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTop 10 Recommended Items:\u001B[39m\u001B[38;5;124m\"\u001B[39m, recommendations)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
